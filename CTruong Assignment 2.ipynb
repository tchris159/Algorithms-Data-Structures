{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02: Web Scraping\n",
    "Your Name:  Christopher Truong\n",
    "Your Class: INST 447  \n",
    "Your Section: MWF 0101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are Amazon reviews fake?\n",
    "Should we trust Amazon reviews? \n",
    "\n",
    "That is a question that seems like we could answer if we only had enough data. So, lets collect some data. Your assignment is to scrape the reviews for five (5) similar products and to save those reviews into a sqllite database, that I have provided. The database will store:\n",
    "\n",
    "**Product Table:**  \n",
    "product_id - int - Primary key (this auto-increments).  \n",
    "amazon_identifier - text - The identifier for the product.  \n",
    "product_name - text - The name of the product.  \n",
    "product_price - text - The price of the product. (Text because sometimes it is a range)  \n",
    "scraper_name - text - Your name...  \n",
    "\n",
    "**Review Table:**  \n",
    "review_id - int - Primary key (this auto-increments).  \n",
    "review_date - text - The date of the review.  \n",
    "review_title - text - The title of the review.  \n",
    "number_of_stars - int - The number of stars the review gave.  \n",
    "verified_purchase - bool - Was the it a \"Verifed Purchase\"?  \n",
    "review_body - text - The text of the review.  \n",
    "number_found_helpful - int - The number of people that found the review helpful.  \n",
    "product_id - int - Foreign key for the product.  \n",
    "\n",
    "Since you are using my database structure, this means that I can run your code and fill up a single database with all of our data.\n",
    "\n",
    "** YOU DO NOT HAVE TO DO ANYTHING WITH amazon-page_dump.db IF YOU ARE GETTING RESPONSES FROM AMAZON ** \n",
    "I have also given you a database named 'amazon-page_dump.db'. If you keep getting errors from Amazon, then tell me what that error is including the status-code, the reason given, and what you think that error means. Then use the pages saved in the page_dump table to complete the assignment. You will have to figure out how to best adapt the code framework I gave you to pull out the pages to parse.\n",
    "\n",
    "**Database: amazon-page_dump.db  \n",
    "Table: page_dump**  \n",
    "dump_id - integer - Primary key (this auto-increments)  \n",
    "amazon_identifier - text - The Amazon product identifier  \n",
    "page_url - text - The url for the page  \n",
    "page_html - text - The html of the page  \n",
    "\n",
    "This is only in case you are getting only errors from Amazon and cannot access the pages to scrape. ** YOU DO NOT HAVE TO DO ANYTHING WITH amazon-page_dump.db IF YOU ARE GETTING RESPONSES FROM AMAZON ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the amazon.db database, if it does not exist.\n",
    "conn = sqlite3.connect('amazon.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x27c7b99b180>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the products table \n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        product_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        amazon_identifier TEXT,\n",
    "        product_name TEXT,\n",
    "        product_price TEXT,\n",
    "        scraper_name TEXT\n",
    "        );\n",
    "''')\n",
    "# Create the reviews table\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS reviews (\n",
    "        review_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        review_date TEXT, \n",
    "        review_title TEXT, \n",
    "        number_of_stars INTEGER, \n",
    "        verified_purchase BOOLEAN, \n",
    "        review_body TEXT, \n",
    "        number_found_helpful INTEGER,\n",
    "        product_id INTEGER,\n",
    "        FOREIGN KEY(product_id) REFERENCES products(product_id)\n",
    "        )\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Find 5 similar products on Amazon that have more than 5 reviews each (my example uses hotsauce, so you can't). Your products must be PG.\n",
    "Grab their product identifiers and replace mine in the list I have below.\n",
    "\n",
    "It is in the URL and it looks like these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# product_lists = ['B06Y4KR6FS', 'B01GXAT0BK', 'B01LEX4UPW', 'B01LY0QOPZ', 'B00137QZQW']\n",
    "product_lists = ['B06Y4KR6FS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Urls for the reviews look like: https://www.amazon.com/product-reviews/B00AIR3Q38/?reviewerType=all_reviews&pageNumber=1\n",
    "\n",
    "Notice that there is a spot in the url where the Amazon identifier goes and that there is an argument to be able to set the number of the page that is accesed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# URL template\n",
    "url = 'https://www.amazon.com/product-reviews/%s/'\n",
    "# Default URL arguments\n",
    "url_args = {'reviewerType': 'all_reviews',\n",
    "            'pageNumber': 1,\n",
    "            'sortBy': 'recent'}\n",
    "# Pretend to be a browser\n",
    "headers = {'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through the identifiers and save the specified data into the database.\n",
    "\n",
    "I have provided a shell. Fill out the bits that have comments that are like:\n",
    ">#TASK: Get (x) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reviews(review_soup):\n",
    "    '''Will return a list of dictionaries with data for each review'''\n",
    "    reviews = []\n",
    "    review_tags = [item for item in review_soup.find_all('div', class_='a-section review') if \"data-hook\" in item.attrs]\n",
    "    for review_tag in review_tags:\n",
    "        review = {}\n",
    "        review['stars'] = float(review_tag.find('span', class_='a-icon-alt').get_text().split(' ')[0])\n",
    "        review['date'] = review_tag.find('span', class_='a-size-base a-color-secondary review-date').get_text().replace('on ', '')\n",
    "        review['title'] = review_tag.find('a', class_='a-size-base a-link-normal review-title a-color-base a-text-bold').get_text()\n",
    "        review['body'] = review_tag.find('span', class_='a-size-base review-text').get_text()\n",
    "        review['found_helpful'] = 0\n",
    "        if review_tag.find('span', class_='a-size-base a-color-secondary cr-vote-text') != None:\n",
    "            # this is the tag to find found helpful, you will have to handle this differently\n",
    "            if review_tag.find('span', class_='a-size-mini a-color-state a-text-bold').get_text() == 'Verified Purchase':\n",
    "                review['verified'] = True\n",
    "        else:\n",
    "            review['verified'] = False\n",
    "        reviews.append(review)\n",
    "        print(review)\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a2fd8c297c31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Get the first page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0murl_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pageNumber'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mamazon_identifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop through the Amazon product identifiers\n",
    "for amazon_identifier in product_lists:\n",
    "    # Get the first page\n",
    "    url_args['pageNumber'] = 1\n",
    "    r = requests.get(url % amazon_identifier, url_args)\n",
    "    print(r.url)\n",
    "    \n",
    "    # Check if the request was successful.\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        page_soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "        \n",
    "        # TASK: Get the number of the last page and save it into the variable max_page number\n",
    "        # note: make sure you handle the cases where there is a last button and when there isn't a last button\n",
    "        max_page_number = 65\n",
    "        \n",
    "        # TASK: Get the product namea nd save to the variable below\n",
    "        product_name = \"Apple iPhone SE 16 GB Factory Unlocked, Space Gray (Certified Refurbished)\"\n",
    "        \n",
    "        # TASK: Get the product price and save to the variable below\n",
    "        product_price = ''\n",
    "        \n",
    "        # TASK: Change this to your name\n",
    "        scraper_name = 'Chris'\n",
    "        \n",
    "        # Try to insert it. If this does not work, then we won't have a product_id to associate the reviews with and shouldn't save them.\n",
    "        try:\n",
    "            c.execute('INSERT INTO products (amazon_identifier, product_name, product_price, scraper_name) VALUES (?, ?, ?, ?)', \n",
    "                      (amazon_identifier, product_name, product_price, scraper_name))\n",
    "            # We have to commit the transaction, or it won't be saved.\n",
    "            conn.commit()\n",
    "            # Save the last primary key inserted as the product_id\n",
    "            product_id = c.lastrowid\n",
    "\n",
    "            # If there are more than 5 pages, then stop at 5. \n",
    "            if max_page_number > 5:\n",
    "                max_page_number = 5\n",
    "            \n",
    "            # Loop through all of the pages (1 through max)\n",
    "#             for page in range(1, max_page_number): \n",
    "            for page in range(1,2):\n",
    "                # Set the page number for the url\n",
    "                url_args['pageNumber'] = page\n",
    "                # Get the next page of reviews\n",
    "                r = requests.get(url % amazon_identifier, url_args)\n",
    "                print(r.url)\n",
    "                # Check if we got a response\n",
    "                if r.status_code == requests.codes.ok:\n",
    "                    review_soup = BeautifulSoup(r.content, 'lxml')\n",
    "                    reviews = get_reviews(review_soup)\n",
    "                    \n",
    "                    # TASK: Loop through the reviews (replace the [] with your code)\n",
    "                    for review in reviews:\n",
    "                        \n",
    "                        # TASK: Get the review date - convert it if necessary to YYYY-MM-DD\n",
    "                        review_date = review['date']\n",
    "                        \n",
    "                        # TASK: Get the review title\n",
    "                        review_title = review['title']\n",
    "                        \n",
    "                        # TASK: Get the number of stars - and make sure it is an int.\n",
    "                        number_of_stars = review['stars']\n",
    "                        \n",
    "                        # TASK: Get whether it is a verified purchase or not\n",
    "                        verified_purchase = review['verified']\n",
    "                            \n",
    "                        # TASK: Get the actual text of the review\n",
    "                        review_body = review['body']\n",
    "                        \n",
    "                        # TASK: Get the number of people that found the review helpful\n",
    "                        number_found_helpful = review['found_helpful']\n",
    "                        \n",
    "                        # Try to insert the review into the database. If it doesn't work. Then tell us why.\n",
    "                        try:\n",
    "                            c.execute('''INSERT INTO reviews \n",
    "                                            (product_id, review_date, review_title, number_of_stars, verified_purchase, review_body, number_found_helpful) \n",
    "                                         VALUES (?, ?, ?, ?, ?, ?, ?)''', \n",
    "                                      (product_id, review_date, review_title, number_of_stars, verified_purchase, review_body, number_found_helpful))\n",
    "                            conn.commit()    \n",
    "                        except sqlite3.DatabaseError as err:\n",
    "                            print('SQL Error: {0}'.format(err))\n",
    "                else: \n",
    "                    print('Error %s for %s on page %s' % (r.status_code, amazon_identifier, page))\n",
    "                    \n",
    "                # Slow things down.\n",
    "                time.sleep(0.5)\n",
    "        except sqlite3.DatabaseError as err:\n",
    "            print('SQL Error: {0}'.format(err))\n",
    "    else:\n",
    "        print('Error %s for %s' % (r.status_code, amazon_identifier))\n",
    "    # Slow things down.\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to see if there are products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('SELECT COUNT(*) FROM products;')\n",
    "c.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to see if there are reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('SELECT COUNT(*) FROM reviews;')\n",
    "c.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
